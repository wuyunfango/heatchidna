---
title: "Exploring bushfire risk across Victoria"
format: revealjs
editor: visual
---

## 

## Exploring bushfire risk across Victoria with open data sources and open source software

By Yunfang Wu

Under the supervision of Professor Di Cook and Dr Kate Saunders

```{r}
library(ggplot2)
library(tidyverse)
library(here)
library(rpart)
library(rpart.plot)
library(randomForest)
library(knitr)
library(randomForest)
```

## Motivation

Victoria, Australia is one of the most fire-prone regions in the world - given its fire-conducive weather and fuel conditions.

::: columns
::: {.column width="50%"}
-   Understand risk of fire in different area of Victoria

-   Provide guidance to allocating resource to areas with higher fire risk

-   Develop strategies to mitigate fire risk
:::

::: {.column width="50%"}
![](data/hotspots.gif)
:::
:::

::: notes
Bushfires are a common and natural phenomenon that frequently occurs in many places around the world.

Victoria, Australia is one of the most fire-prone regions in the world - given its fire-conducive weather and fuel conditions. 

in the past including, horrific events had happened here in Victoria.

The animation here shows historical fire ignition points in Victoria gathered by the Japanese [Himawari-](http://www.bom.gov.au/australia/satellite/himawari.shtml)8 satellite.

It is important to understand and the risk of bushfires as understanding the risk of fire in different areas of Victoria would help in developing strategies for mitigating the risks. 

Effective bush fire risk indicator would provides relevant and timely data to both the emergency personnel and the public to mitigate fire risk

and allowing them to allocating resources to where its needed.
:::

## Research Questions/ Contribution of the thesis

What part of Victoria has higher bush fire risk?

-   To allocate resources wisely

What factors are the more important for predicting the risk accurately?

-   To understand how different factor associate with fire risk

How change in factors affect bush fire risk of a particular location?

-   Provide on guidance on mitigating fire risk under a particular condition

::: notes
To put this in context, I focused on there three research questions as the contribution of this thesis

to allow resource allocation, the first question is What part of Victoria has higher bush fire risk?
:::

## Methodology

![](data/matheq.png){width="152"}

Construction of 2D kernel density estimate. Left. Individual kernels. Right. Kernel density estimate.

![](data/2d.png){width="574"}

::: notes
2 dimentional kernal density estimation is used to understand the which part of Victoria has higher fire risk compare to other parts in Victoria.

A [non-parametric](https://en.wikipedia.org/wiki/Non-parametric_statistics "Non-parametric statistics") method to [estimate](https://en.wikipedia.org/wiki/Estimation "Estimation") the [probability density function](https://en.wikipedia.org/wiki/Probability_density_function "Probability density function") of a [random variable](https://en.wikipedia.org/wiki/Random_variable "Random variable") based on [*kernels*](https://en.wikipedia.org/wiki/Kernel_(statistics) "Kernel (statistics)") as [weights](https://en.wikipedia.org/wiki/Weight_function "Weight function").

non-parametric meaning we make less assumption about the distribution of data, but instead we determined the probability distribution directly from the observed data.

I our case, we treat fire hot spot location as a random variable, and we are estimating the probability distribution based on a sample data set of historical fire ignition points in Victoria.

A kernal density estimate takes two parameters, K , that is a kernal density forumula, this can be any probability density distribution that is symmetric, for example, the circles on the left hand side represent a 2D Gaussian distribution. A kernel will be place on each observed point.

The H bandwidth, is a matrix that decide the area where we want to add up the Gaussian distribution.

For estimating fire hot spot density, Base on our choice of bandwidth, for hot spots within the bandwidth value, the probability distribution kernels will be added together and divided by the number of points with the bandwidth, to compute the overall probability density of that area.

In practice, the choice of kernel does not affect the overall density estimation much.

However, the bandwidth matters.

To better illustrate the effect of different bandwidth choice on the density, lets look at an example.
:::

## Methodology

A [non-parametric](https://en.wikipedia.org/wiki/Non-parametric_statistics "Non-parametric statistics") method to [estimate](https://en.wikipedia.org/wiki/Estimation "Estimation") the [probability density function](https://en.wikipedia.org/wiki/Probability_density_function "Probability density function") of a [random variable](https://en.wikipedia.org/wiki/Random_variable "Random variable") based on [*kernels*](https://en.wikipedia.org/wiki/Kernel_(statistics) "Kernel (statistics)") as [weights](https://en.wikipedia.org/wiki/Weight_function "Weight function").

**Example of 2D density estimation:**

-   Here is a set of randomly generated points

```{r}
set.seed(123)
x = runif(100)
y = runif(100)
plot(x,y)

```

::: notes
Here is a set of randomly generated points,

there are 100 points with coordinate x and y
:::

## **Example of 2D density estimation:**

x:0.48, y: 0.44

```{r}
library(MASS)
kde <- kde2d(x,y,n=100)
# Library
library(plotly)
p <- plot_ly(z = kde$z, type = "surface")
p 
```

::: notes
Here is the estimated density as a 3d object.

The kernel used here is a Gaussian kernel, and the bandwidth is 0.48 on the x axis and 0.44 on the y axis

Since our data is two dimensional, the overall density estimation for the 2 dimensional data will become 3 dimensional.

the third dimension Z represent the estimated density of the corresponding point(x,y)

This diagram is relatively smooth since our bandwidth is quite large.

Now lets compare result of density estimate using different bandwidth.
:::

## 

## Comparison of Bandwidth:

::: columns
::: {.column width="50%"}
Bandwidth: (0.5,0.5)

![](data/kde1.png){width="548"}
:::

::: {.column width="50%"}
Bandwidth: (1,1)

![](data/kde2.png){width="547"}
:::
:::

::: notes
Here are two density estimate on for the randomly generated points,

It represent density estimate in the format of a heat map.

More red meaning the area has a higher density.

we can see that the one with bandwidth 1,1 on the right is much smoother than the lefe one.

Overall we can see that a smaller bandwidth will result more wiggly estimate; which means the estimated density varies more for a given size of area.
:::

## Result of 2D Density Estimation

-   Bias of density estimation of edges (or frontiers) of regions

    -   Broader bias

    -   Ripley's correction

::: columns
::: {.column width="50%"}
![](data/result.png){width="587" height="415"}
:::

::: {.column width="50%"}
![](data/nc.png){width="591" height="420"}
:::

Bandwidth(0.02606,0.02606) , which is around 2.90 kilometers in radius.
:::

::: notes
The Bias of density estimation of edges of regions also need to take account.

Bias of density estimation of edges happens because the fact that we do not have what the density look like outside the map, but still need to average the density base areas outside the map, because the way 2 d density estimation works.

Therefore, to address this problem, a correction is used by mirroring the data points inside the map to the outside area, then calculate the density with the added buffer area.

((compare the graph))
:::

## Result of 2D Density Estimation (2)

Increase the bandwidth to 8.7 kilometers in radius.

![](data/bigh.png){width="588"}

::: notes
Looking at the plot above,

we notice that the range of the density has been reduced, it is not 0\~0.369,

where previously when our bandwidth is 0\~0.532, when the bandwidth was about 2.9 km in radius.

As bandwidth parameter increase, the overall density estimation is more smooth
:::

## Result of 2D Density Estimation (3)

Decrease the bandwidth to 1.45 kilometers in radius.

![](data/smallh.png){width="583"}

::: notes
lets now compare the result from the different choice of bandwidth parameter. Comparison of result

I decided to go with the first band width, it gives more detail than using a 8km but better than 1.45km where the result are really wiggly
:::

## Modeling

### Three Model

![](data/tree.png){width="658"}

::: notes
Now we have estimated the risk variable, we will use many different environmental variables, as well as the forest type, and proxy to human activity data to find relationship between the estimated density and the variables we have.

The model we choose to is a random forest model

To better understand how a random forest model works, we can take a look at the tree model.

here is a fitted tree model--- exaplain

**Parent and Child Node:** A node that gets divided into sub-nodes is known as Parent Node,

and these sub-nodes are known as Child Nodes. Since a node can be divided into multiple sub-nodes, therefore a node can act as a parent node of numerous child nodes

**Root Node:** The top-most node of a decision tree. It does not have any parent node. It represents the entire population or sample

**Leaf / Terminal Nodes:** Nodes that do not have any child node are known as Terminal/Leaf Nodes
:::

## Tree Model

How did tree model decide on which variable and how to split?

::: columns
::: {.column width="70%"}
Node splitting, or simply splitting, is the process of dividing a node into multiple sub-nodes to create relatively pure nodes.

This process is performed multiple times during the training process until only homogeneous nodes are left.

-   **Reduction in Variance**

![](https://cdn.analyticsvidhya.com/wp-content/uploads/2020/06/ns2.png){fig-align="center" width="211"}
:::

::: {.column width="30%"}
![](images/Root%20Node.webp)
:::
:::

::: notes
Now lets look at the concept of

A decision tree makes decisions by splitting nodes into sub-nodes. This process is performed multiple times during the training process until only homogeneous nodes are left. Node splitting, or simply splitting, is the process of dividing a node into multiple sub-nodes to create relatively pure nodes.

This process is performed multiple times during the training process until only homogeneous nodes are left. 

For predicting continuous output , Reduction in Variance is used to decide node purity

1.  For each split, individually calculate the variance of each child node

2.  Calculate the variance of each split as the weighted average variance of child nodes

3.  Select the split with the lowest variance

4.  Perform steps 1-3 until completely homogeneous nodes are achieved

Disadvantages of trees

Tress are very sensitive to small change in training data

Low variability, Trees can have higher variability across different training data sets.
:::

## Random Forest model

::: columns
::: {.column width="50%"}
1.  Decide number of trees to fit, n
2.  Bootstrap samples from original data set n times, creating n training and testing sets
3.  Fit a decision tree to each set of training set
4.  Predict result by averaging all predicted values base on all trees
:::

::: {.column width="50%"}
![](data/forest.png)
:::
:::

::: notes
Our choice of the model is a random forest model

Random Forest model address the disadvantage of tree model, as The predicted results is an average of different decision trees, the Random forest algorithm **avoids and prevents over fitting by using multiple trees**. 

From the process, we can see there are two feature of randomness:

we used bootstrap sample and each tree is a different data sample drawn from a training set with replacement.

Another instance of randomness is called feature bagging, where it **randomly selects subsets of features used in each data sample**, which add more diversity to the dataset and reducing the correlation among decision trees

Finally, the oob sample is then used for cross-validation, finalizing that prediction. 
:::

## Fitting the Random Forest model

MSE of tree model: 0.0038

```{r}
#| echo: true
sample_hotspots <-read_csv(here::here("model_df.csv"))
set.seed(456)
RFM = randomForest(density_result~., 
                   data = sample_hotspots,
                   importance=TRUE)

print(RFM)
```

::: notes
Builds *500* decision tree regressors (estimators). The number of estimators *n* [defaults to 500 in](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html)R, where it is called *n_estimators*. The trees are built following the specified hyperparameters (e.g. minimum number of samples at the leaf nodes, maximum depth that a tree can grow, etc.).

Average prediction across estimators. Each decision tree regression predicts a number as an output for a given input. Random forest regression takes the average of those predictions as its 'final' output.

(look at the result tree)

the MSE shows the average mean squared residuals across 500 different testing sets
:::

## Fitting the Bigger Random Forest

```{r}
#| echo: true
set.seed(222)
RFM_BIG = randomForest(density_result~.,
                  #change number of trees to 1000
                   ntree =1000, 
                  #try 25 variables at each node
                  mtry = 14, 
                  data = sample_hotspots,
                  importance=TRUE)

print(RFM_BIG)
```

::: notes
This time, we change the default parameter, and are fitting 1000 trees,

there is a reduction in the MEAN of squared error, previously it was 0.001337

Data description:

FOR_TYPE: Forest type. Eg. Acacia, Callitris, Casuarina, etc.

COVER: Forest crown cover

HEIGHT: Forest height class

arf360: Average rainfall in the past 360 days in mm

ase90: Average global solar exposure in the past 90 days - MJ/m\^2

ase180: Average global solar exposure in the past 180 days - MJ/m\^2

ase720: Average global solar exposure in the past 720 days - MJ/m\^2

amaxt90: Average maximum temperature in the past 90 days - Celsius degree

amaxt180: Average maximum temperature in the past 180 days - Celsius degree

amaxt720: Average maximum temperature in the past 720 days - Celsius degree

amint180: Average minimum temperature in the past 180 days - Celsius degree

ws: Average wind speed on that day - m/s

aws_m12: Average wind speed in last 12 months - m/s

aws_m24: Average wind speed in last 24 months - m/s

log_dist_cfa: Natural logarithm of the distance to the nearest CFA station - m

log_dist_camp: Natural logarithm of the distance to the nearest recreation site - m

log_dist_road: Natural logarithm of the distance to the nearest road - m
:::

## Variable importance

![](data/varimp.png){width="623"}

%IncMSE of j'th is (mse(j)-mse0)/mse0 \* 100%

::: notes
Now lets take a look at the variable importance plot, this plot ranks the importance of different variables in the random forest model makes the prediction.

%IncMSE is the most robust and informative measure. It is the increase in mse of predictions as a result of a variable values randomly shuffled.

1.  grow regression forest. Compute OOB-mse, name this mse0.

2.  for all variables, permute their values, then predict and compute the MSE on testing sets

3.  the formula shows how the %Increase MSE is calculated

Increase in NodePurity is similar to the steps measuring %IncMSE,

where impurity is measured as sum of squared deviations from the mean of the terminal node.
:::

## Variable importance

![](relationship.png){width="672"}

::: notes
this is a group of scatter plot of some of the most important variables in used in the Random Forest modle.

form the plot, we cannot see obvious relationships between --- and ---

amaxt90: Average maximum temperature in the past 90 days - Celsius degree

amint720: Average maximum temperature in the past 720 days - Celsius degree

arf720: Average maximum temperature in the past 720 days -

aws_m24: Average wind speed in last 24 months - m/s

log_dist_cfa: Natural logarithm of the distance to the nearest CFA station - m

log_dist_camp: Natural logarithm of the distance to campsites
:::

## Effects of Change in Variables Values to risk prediction

Proxy to human activity

```{r}
#| echo: false
example<-sample_hotspots[1,]
```

```{r}
#| echo: true
previouse_risk<- example$density_result
example$dist_camp <- 150000
new_risk<-predict(RFM_BIG, example)
cat("previous risk is: ", previouse_risk,"with increase in dist_to_camp", 
      "\\n new risk prediction is : " ,new_risk)
```

```{r}
#| echo: false
example<-sample_hotspots[1,]
```

```{r}
#| echo: true
previouse_risk<- example$density_result
example$dist_cfa <- 50000
new_risk<-predict(RFM_BIG, example)
cat("Previous risk is: ", previouse_risk,"with increase in dist_to_cfa", 
      ",\nNew risk prediction becomes : " ,new_risk)
```

## Effects of Change in Variables Values to risk prediction

Enviromental variables

```{r}
#| echo: false
example<-sample_hotspots[1,]$arf720
example
```

```{r}
#| echo: true
previouse_risk<- example$density_result
example$amaxt60 <- 40
new_risk<-predict(RFM_BIG, example)
cat("Previous risk is: ", previouse_risk,", changing amaxt60 to",
    example$amaxt60,
      ",\nNew risk prediction becomes : " ,new_risk)
```

```{r}
#| echo: false
example<-sample_hotspots[1,]
example
```

```{r}
#| echo: true
previouse_risk<- example$density_result
example$arf7 <- 1
new_risk<-predict(RFM, example)
cat("Previous risk is: ", previouse_risk,", changing amrf720 to",
    example$arf7,
      ",\nNew risk prediction becomes : " ,new_risk)
```
