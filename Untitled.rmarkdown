---
title: "Exploring bushfire risk across Victoria"
format: revealjs
editor: visual
---


## 

## Exploring bushfire risk across Victoria 

By Yunfang Wu

Under the supervision of Professor Di Cook and Dr Kate Saunders


```{r}
library(ggplot2)
library(tidyverse)
library(here)
library(rpart)
library(rpart.plot)
library(randomForest)
library(knitr)
library(randomForest)
```


## Motivation

Victoria, Australia is one of the most fire-prone regions in the world - given its fire-conducive weather and fuel conditions.

::: columns
::: {.column width="50%"}
-   Understand risk of fire in different area of Victoria

-   Provide guidance to allocating resource to areas with higher fire risk

-   Develop strategies to mitigate fire risk
:::

::: {.column width="50%"}
![](data/hotspots.gif)
:::
:::

::: notes
Bushfires are a common and natural phenomenon that frequently occurs in many places around the world.

 Victoria, Australia is one of the most fire-prone regions in the world - given its fire-conducive weather and fuel conditions. 

in the past including, horrific events had happened here in Victoria.

The gif here shows all historical fire ignition points in Victoria gathered by the Japanese [Himawari-](http://www.bom.gov.au/australia/satellite/himawari.shtml)8 satellite.

It is important to understand and the risk of bushfires as understanding the risk of fire in different areas of Victoria would help in developing strategies for mitigating the risks. 

Effective bush fire risk indicator would provides relevant and timely data to both the emergency personnel and the public to mitigate fire risk by allocating resources to where its needed.
:::

## Research Questions

What part of Victoria has higher bush fire risk?

-   To allocate resources wisely

What factors are the more important for predicting the risk accurately?

-   To understand how different factor associate with fire risk

How change in factors affect bush fire risk of a particular location?

-   Provide on guidance on mitigating fire risk under a particular condition

::: notes
To put this in context, I focused on there three research questions.
:::

## Methodology

::: columns
::: {.column width="70%"}
**2 Dimensional Kernel density estimation**
:::

::: {.column width="30%"}
![](data/matheq.png){fig-align="left" width="259"}
:::

::: {.column width="100%"}
**n** is the number of items in each vector x and y, in our case the longitude and latitude

**x** = (*x*~1~, *x*~2~, ..., *x~d~*)^*T*^, x~*i*~= (*x~i~*~1~, *x~i~*~2~, ..., *x~id~*)^*T*^, *i* = 1, 2 are vectors representing x and y values

**H** is the bandwidth (or smoothing): *2×2* matrix which is [symmetric](https://en.wikipedia.org/wiki/Symmetric_matrix "Symmetric matrix") and [positive definite](https://en.wikipedia.org/wiki/Positive_definite_matrix "Positive definite matrix")

***K*** is the [kernel](https://en.wikipedia.org/wiki/Kernel_(statistics) "Kernel (statistics)") function which is a symmetric multivariate density
:::
:::

::: notes
A [non-parametric](https://en.wikipedia.org/wiki/Non-parametric_statistics "Non-parametric statistics") method to [estimate](https://en.wikipedia.org/wiki/Estimation "Estimation") the [probability density function](https://en.wikipedia.org/wiki/Probability_density_function "Probability density function") of a [random variable](https://en.wikipedia.org/wiki/Random_variable "Random variable") based on [*kernels*](https://en.wikipedia.org/wiki/Kernel_(statistics) "Kernel (statistics)") as [weights](https://en.wikipedia.org/wiki/Weight_function "Weight function").

non-parametric meaning we make less assumption about the distribution of data, but instead we determined the probability distribution directly from the available observed data.

I our case, we treat fire hot spot location as a random variable, and we are estimating the probability distribution based on a sample data set of historical fire ignition points in Victoria.

The kernel K can be a 2D Gaussian distribution. A kernal density will be place on each observed hot spot.

Base on our choice of bandwidth, for hot spots within the bandwidth value K, the probability distribution kernels will be added together and divided by the number of points with the bandwidth, to compute the overall probability density of that area.

In practice, the choice of kernel does not affect the overall density estimation much.

However, the bandwidth matters.
:::

## Methodology

A [non-parametric](https://en.wikipedia.org/wiki/Non-parametric_statistics "Non-parametric statistics") method to [estimate](https://en.wikipedia.org/wiki/Estimation "Estimation") the [probability density function](https://en.wikipedia.org/wiki/Probability_density_function "Probability density function") of a [random variable](https://en.wikipedia.org/wiki/Random_variable "Random variable") based on [*kernels*](https://en.wikipedia.org/wiki/Kernel_(statistics) "Kernel (statistics)") as [weights](https://en.wikipedia.org/wiki/Weight_function "Weight function").

**Example of 2D density estimation:**

-   Here is a set of randomly generated points


```{r}
set.seed(123)
x = runif(100)
y = runif(100)
plot(x,y)

```


::: notes
To better illustrate the effect of different bandwidth choice on the density, lets look at an example.

Here is a set of randomly generated points,

there are 100 points with coordinate x and y
:::

## **Example of 2D density estimation:**

x:0.48, y: 0.44


```{r}
library(MASS)
kde <- kde2d(x,y,n=100)
# Library
library(plotly)
p <- plot_ly(z = kde$z, type = "surface")
p 
```


::: notes
Here is the estimated density.

Since our data is two dimensional, the overall density estimation for the 2 dimensional data will be 2 dimensional.

-   then show the plot and explain
:::

## 

## Comparison of Bandwidth:

::: columns
::: {.column width="50%"}
Bandwidth: (0.5,0.5)

![](data/kde1.png){width="548"}
:::

::: {.column width="50%"}
Bandwidth: (1,1)

![](data/kde2.png){width="547"}
:::
:::

## Boundary Bias

## Result of 2D Density Estimation

-   Bias of density estimation of edges (or frontiers) of regions

    -   Broader bias

    -   Ripley's correction

::: columns
::: {.column width="50%"}
![](data/result.png){width="587" height="415"}
:::

::: {.column width="50%"}
![](data/nc.png){width="591" height="420"}
:::

Bandwidth(0.02606,0.02606) , which is around 2.90 kilometers in radius.
:::

## Result of 2D Density Estimation (2)

Increase the bandwidth to 8.7 kilometers in radius.

![](data/bigh.png){width="588"}

::: notes
Looking at the plot above,

we notice that the range of the density has been reduced, it is not 0\~0.369,

where previously when our bandwidth is 0\~0.532, when the bandwidth was about 2.9 km in radius.

As bandwidth parameter increase, the overall density estimation is more smooth
:::

## Result of 2D Density Estimation (3)

Decrease the bandwidth to 1.45 kilometers in radius.

![](data/smallh.png){width="583"}

::: notes
lets now compare the result from the different choice of bandwidth parameter.

For the first one
:::

Comparison of result

## Modeling

### Three Model

![](data/tree.png){width="658"}

::: notes
Now we have estimated the risk variable

To better understand how a random forest model works, we can take a look at the tree model.

**Parent and Child Node:** A node that gets divided into sub-nodes is known as Parent Node, and these sub-nodes are known as Child Nodes. Since a node can be divided into multiple sub-nodes, therefore a node can act as a parent node of numerous child nodes

**Root Node:** The top-most node of a decision tree. It does not have any parent node. It represents the entire population or sample

**Leaf / Terminal Nodes:** Nodes that do not have any child node are known as Terminal/Leaf Nodes
:::

## Tree Model

How did tree model decide on which variable and how to split?

::: columns
::: {.column width="70%"}
Node splitting, or simply splitting, is the process of dividing a node into multiple sub-nodes to create relatively pure nodes.

This process is performed multiple times during the training process until only homogeneous nodes are left.

-   **Reduction in Variance**

![](https://cdn.analyticsvidhya.com/wp-content/uploads/2020/06/ns2.png){fig-align="center" width="211"}
:::

::: {.column width="30%"}
![](images/Root%20Node.webp)
:::
:::

::: notes

A decision tree makes decisions by splitting nodes into sub-nodes. This process is performed multiple times during the training process until only homogenous nodes are left. 

For predicting continuous output , 

Reduction in Variance is used to decide node purity

1.  For each split, individually calculate the variance of each child node

2.  Calculate the variance of each split as the weighted average variance of child nodes

3.  Select the split with the lowest variance

4.  Perform steps 1-3 until completely homogeneous nodes are achieved

Disadvantages of trees

Tress are very sensitive to small change in training data

Low variability
Trees can have higher variability across different training data sets.
:::

## Random Forest model

::: columns
::: {.column width="50%"}
1.  Decide number of trees to fit, n
2.  Bootstrap samples from original data set n times, creating n training and testing sets
3.  Fit a decision tree to each set of training set
4.  Predict result by averaging all predicted values base on all trees
:::

::: {.column width="50%"}
![](data/forest.png)
:::
:::

::: notes
Random Forest model

The random forest algorithm is made up of a collection of decision trees, and each tree in the ensemble is comprised of a data sample drawn from a training set with replacement, called the bootstrap sample. 

Another instance of randomness is then injected through feature bagging by **randomly selects subsets of features used in each data sample**, adding more diversity to the dataset and reducing the correlation among decision trees

Of that training sample, one-third of it is set aside as test data, known as the out-of-bag (oob) sample. 

Depending on the type of problem. For predicting continuous variable, the individual decision trees will be averaged.

Finally, the oob sample is then used for cross-validation, finalizing that prediction. 

The predicted results is an average of different decision trees, the Random forest algorithm **avoids and prevents overfitting by using multiple trees**. 
:::

## Fitting the Random Forest model

MSE of tree model: 0.0038


```{r}
#| echo: true
sample_hotspots <-read_csv(here::here("model_df.csv"))
set.seed(456)
RFM = randomForest(density_result~., 
                   data = sample_hotspots,
                   importance=TRUE)

print(RFM)
```


::: notes


Builds *500* decision tree regressors (estimators). The number of estimators *n* [defaults to 500 in ](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html)R, where it is called *n_estimators*. The trees are built following the specified hyperparameters (e.g. minimum number of samples at the leaf nodes, maximum depth that a tree can grow, etc.).

Average prediction across estimators. Each decision tree regression predicts a number as an output for a given input. Random forest regression takes the average of those predictions as its \'final\' output.

(look at the result tree)

the MSE shows the average mean squared residuals across 500 different testing sets
:::

## Fitting the Bigger Random Forest


```{r}
#| echo: true
set.seed(222)
RFM_BIG = randomForest(density_result~.,
                  #change number of trees to 1000
                   ntree =1000, 
                  #try 25 variables at each node
                  mtry = 25, 
                  data = sample_hotspots,
                  importance=TRUE)

print(RFM_BIG)
```


::: notes
This time, we change the default parameter, and are fitting 1000 trees,

there is a reduction in the MEAN of squared error, previously it was 0.001337

Data description:

FOR_TYPE: Forest type. Eg. Acacia, Callitris, Casuarina, etc.

COVER: Forest crown cover

HEIGHT: Forest height class

arf360: Average rainfall in the past 360 days in mm

ase90: Average global solar exposure in the past 90 days - MJ/m\^2

ase180: Average global solar exposure in the past 180 days - MJ/m\^2

ase720: Average global solar exposure in the past 720 days - MJ/m\^2

amaxt90: Average maximum temperature in the past 90 days - Celsius degree

amaxt180: Average maximum temperature in the past 180 days - Celsius degree

amaxt720: Average maximum temperature in the past 720 days - Celsius degree

amint180: Average minimum temperature in the past 180 days - Celsius degree

ws: Average wind speed on that day - m/s

aws_m12: Average wind speed in last 12 months - m/s

aws_m24: Average wind speed in last 24 months - m/s

log_dist_cfa: Natural logarithm of the distance to the nearest CFA station - m

log_dist_camp: Natural logarithm of the distance to the nearest recreation site - m

log_dist_road: Natural logarithm of the distance to the nearest road - m
:::

## Variable importance

![](data/varimp.png){width="623"}

%IncMSE of j'th is (mse(j)-mse0)/mse0 \* 100%

::: notes
%IncMSE is the most robust and informative measure. It is the increase in mse of predictions(estimated with out-of-bag-CV) as a result of variable j being permuted(values randomly shuffled).

1.  grow regression forest. Compute OOB-mse, name this mse0.

2.  for 1 to j var: permute values of column j, then predict and compute OOB-mse(j)

3.  %IncMSE of j'th is (mse(j)-mse0)/mse0 \* 100%

Increase in NodePurity is similar to the steps measuring %IncMSE, where impurity is measured as sum os squared deviations from the mean of the terminal node. How pure is each split in the
:::

## Variable importance

![](relationship.png){width="672"}

::: notes
this is a group of scatter plot of some of the most important variables in used in the Random Forest modle.

form the plot, we cannot see obvious relationships between --- and ---

amaxt90: Average maximum temperature in the past 90 days - Celsius degree

amint720: Average maximum temperature in the past 720 days - Celsius degree

arf720: Average maximum temperature in the past 720 days - Celsius degree

aws_m24: Average wind speed in last 24 months - m/s

log_dist_cfa: Natural logarithm of the distance to the nearest CFA station - m

log_dist_camp: Natural logarithm of the distance to campsites
:::

## Effects of Change in Variables Values to risk prediction

Proxy to human activity


```{r}
#| echo: false
example<-sample_hotspots[1,]
```

```{r}
#| echo: true
previouse_risk<- example$density_result
example$dist_camp <- 150000
new_risk<-predict(RFM_BIG, example)
cat("previous risk is: ", previouse_risk,"with increase in dist_to_camp", 
      "\\n new risk prediction is : " ,new_risk)
```

```{r}
#| echo: false
example<-sample_hotspots[1,]
```

```{r}
#| echo: true
previouse_risk<- example$density_result
example$dist_cfa <- 50000
new_risk<-predict(RFM_BIG, example)
cat("Previous risk is: ", previouse_risk,"with increase in dist_to_cfa", 
      ",\nNew risk prediction becomes : " ,new_risk)
```


## Effects of Change in Variables Values to risk prediction

Enviromental variables


```{r}
#| echo: false
example<-sample_hotspots[1,]
```

```{r}
#| echo: true
previouse_risk<- example$density_result
example$amaxt60 <- 40
new_risk<-predict(RFM_BIG, example)
cat("Previous risk is: ", previouse_risk,", changing amaxt60 to",
    example$amaxt60,
      ",\nNew risk prediction becomes : " ,new_risk)
```

```{r}
#| echo: false
example<-sample_hotspots[1,]
```

```{r}
#| echo: true
previouse_risk<- example$density_result
example$amrf720 <- 5
new_risk<-predict(RFM_BIG, example)
cat("Previous risk is: ", previouse_risk,", changing amrf720 to",
    example$amrf720,
      ",\nNew risk prediction becomes : " ,new_risk)
```

